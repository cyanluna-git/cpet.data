"""Metabolic Analysis Service - ëŒ€ì‚¬ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

Power binning, LOESS smoothing, FatMax/Crossover ë§ˆì»¤ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Features:
- Configurable 10W Power Binning (Median/Mean/Trimmed Mean)
- LOESS smoothing with adjustable fraction
- Phase trimming (Rest/Warm-up/Recovery ì œì™¸)
- FatMax (Peak fat oxidation) + Zone (90% MFO) ê³„ì‚°
- Crossover Point (Fat = CHO intersection) ê³„ì‚°
"""

from dataclasses import dataclass, field
from typing import List, Optional, Tuple, Dict, Any, Literal
import math
import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.optimize import brentq
from scipy.stats import trim_mean

try:
    from statsmodels.nonparametric.smoothers_lowess import lowess

    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False
    lowess = None


@dataclass
class ProcessedDataPoint:
    """ì²˜ë¦¬ëœ ë°ì´í„° í¬ì¸íŠ¸"""

    power: float
    fat_oxidation: Optional[float]
    cho_oxidation: Optional[float]
    count: Optional[int] = None  # binned data only
    vo2: Optional[float] = None  # VO2 for relative calculations and VO2 Kinetics chart
    rer: Optional[float] = None  # RER
    vco2: Optional[float] = None  # VCO2 for VO2 Kinetics chart
    hr: Optional[float] = None  # HR for VO2 Kinetics chart
    ve_vo2: Optional[float] = None  # VE/VO2 for VT Analysis chart
    ve_vco2: Optional[float] = None  # VE/VCO2 for VT Analysis chart

    def to_dict(self) -> Dict[str, Any]:
        """ëª¨ë“  í•„ë“œë¥¼ í•­ìƒ í¬í•¨í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í‚¤ ì¡´ì¬ ì—¬ë¶€ ì²´í¬ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•¨"""
        return {
            "power": self.power,
            "fat_oxidation": self.fat_oxidation,
            "cho_oxidation": self.cho_oxidation,
            "count": self.count,
            "vo2": self.vo2,
            "vco2": self.vco2,
            "rer": self.rer,
            "hr": self.hr,
            "ve_vo2": self.ve_vo2,
            "ve_vco2": self.ve_vco2,
        }


@dataclass
class FatMaxMarker:
    """FatMax ë§ˆì»¤ ì •ë³´"""

    power: int  # FatMax ì§€ì  íŒŒì›Œ (W)
    mfo: float  # Maximum Fat Oxidation (g/min)
    zone_min: int  # FatMax zone í•˜í•œ (W)
    zone_max: int  # FatMax zone ìƒí•œ (W)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "power": self.power,
            "mfo": round(self.mfo, 4),
            "zone_min": self.zone_min,
            "zone_max": self.zone_max,
        }


@dataclass
class CrossoverMarker:
    """Crossover ì§€ì  ë§ˆì»¤ ì •ë³´"""

    power: Optional[int]  # Crossover ì§€ì  íŒŒì›Œ (W), ì—†ìœ¼ë©´ None
    fat_value: Optional[float]  # êµì°¨ ì§€ì  FatOx ê°’
    cho_value: Optional[float]  # êµì°¨ ì§€ì  CHOOx ê°’

    def to_dict(self) -> Dict[str, Any]:
        return {
            "power": self.power,
            "fat_value": (
                round(self.fat_value, 4) if self.fat_value is not None else None
            ),
            "cho_value": (
                round(self.cho_value, 4) if self.cho_value is not None else None
            ),
        }


@dataclass
class MetabolicMarkers:
    """ëŒ€ì‚¬ ë§ˆì»¤ ì •ë³´"""

    fat_max: FatMaxMarker
    crossover: CrossoverMarker

    def to_dict(self) -> Dict[str, Any]:
        return {
            "fat_max": self.fat_max.to_dict(),
            "crossover": self.crossover.to_dict(),
        }


@dataclass
class ProcessedSeries:
    """ì²˜ë¦¬ëœ ì‹œê³„ì—´ ë°ì´í„°"""

    raw: List[ProcessedDataPoint]  # ì›ë³¸ ë°ì´í„°
    binned: List[ProcessedDataPoint]  # 10W êµ¬ê°„ í‰ê· /ì¤‘ì•™ê°’
    smoothed: List[ProcessedDataPoint]  # LOESS smoothed
    trend: List[ProcessedDataPoint] = field(default_factory=list)  # Polynomial fit

    def to_dict(self) -> Dict[str, Any]:
        result = {
            "raw": [p.to_dict() for p in self.raw],
            "binned": [p.to_dict() for p in self.binned],
            "smoothed": [p.to_dict() for p in self.smoothed],
        }
        if self.trend:
            result["trend"] = [p.to_dict() for p in self.trend]
        return result


@dataclass
class MetabolismAnalysisResult:
    """ëŒ€ì‚¬ ë¶„ì„ ê²°ê³¼"""

    processed_series: ProcessedSeries
    metabolic_markers: MetabolicMarkers
    warnings: List[str]

    def to_dict(self) -> Dict[str, Any]:
        return {
            "processed_series": self.processed_series.to_dict(),
            "metabolic_markers": self.metabolic_markers.to_dict(),
            "warnings": self.warnings,
        }


@dataclass
class AnalysisConfig:
    """ë¶„ì„ ì„¤ì • dataclass"""

    loess_frac: float = 0.25
    bin_size: int = 10
    aggregation_method: Literal["median", "mean", "trimmed_mean"] = "median"
    trimmed_mean_proportiontocut: float = 0.1  # trimmed_mean ì‚¬ìš©ì‹œ ì–‘ìª½ 10% ì ˆì‚­
    fatmax_zone_threshold: float = 0.90
    exclude_rest: bool = True
    exclude_warmup: bool = True
    exclude_recovery: bool = True
    min_power_threshold: Optional[int] = None  # e.g., 50W ë¯¸ë§Œ ì œì™¸
    max_power_threshold: Optional[int] = None  # e.g., 400W ì´ˆê³¼ ì œì™¸
    non_negative_constraint: bool = True  # ìŒìˆ˜ ê°’ 0ìœ¼ë¡œ í´ë¨í•‘
    # Initial hyperventilation filtering (RER spike at start)
    exclude_initial_hyperventilation: bool = True
    initial_time_threshold: float = 120.0  # seconds - first 2 minutes
    initial_power_threshold: int = 40  # watts - exclude low power data during startup

    # === NEW: Advanced Physiological Corrections ===
    # Gas Transport Lag Correction (muscle â†’ breath delay)
    gas_delay_seconds: float = 15.0  # Typical range: 10-20 seconds
    # Rolling IQR Outlier Detection
    outlier_window_size: int = 30  # Rolling window in seconds
    outlier_iqr_multiplier: float = (
        2.0  # Values outside Median Â± IQR*multiplier are filtered
    )
    # Sparse Data Handling for Trend Lines
    min_points_for_trend: int = 3  # Minimum binned points required in range
    trend_gap_threshold_watts: int = 30  # Max gap before breaking trend line

    def to_dict(self) -> Dict[str, Any]:
        return {
            "loess_frac": self.loess_frac,
            "bin_size": self.bin_size,
            "aggregation_method": self.aggregation_method,
            "fatmax_zone_threshold": self.fatmax_zone_threshold,
            "exclude_rest": self.exclude_rest,
            "exclude_warmup": self.exclude_warmup,
            "exclude_recovery": self.exclude_recovery,
            "min_power_threshold": self.min_power_threshold,
            "max_power_threshold": self.max_power_threshold,
            "non_negative_constraint": self.non_negative_constraint,
            "exclude_initial_hyperventilation": self.exclude_initial_hyperventilation,
            "initial_time_threshold": self.initial_time_threshold,
            "initial_power_threshold": self.initial_power_threshold,
            # New parameters
            "gas_delay_seconds": self.gas_delay_seconds,
            "outlier_window_size": self.outlier_window_size,
            "outlier_iqr_multiplier": self.outlier_iqr_multiplier,
            "min_points_for_trend": self.min_points_for_trend,
            "trend_gap_threshold_watts": self.trend_gap_threshold_watts,
        }


class MetabolismAnalyzer:
    """ëŒ€ì‚¬ ë°ì´í„° ë¶„ì„ê¸°

    Features:
    - Configurable power binning (5W~30W)
    - Multiple aggregation methods (median, mean, trimmed_mean)
    - LOESS smoothing with adjustable fraction
    - Phase trimming options
    - FatMax and Crossover calculation
    """

    def __init__(
        self,
        loess_frac: float = 0.25,
        bin_size: int = 10,
        use_median: bool = True,
        fatmax_zone_threshold: float = 0.90,
        config: Optional[AnalysisConfig] = None,
    ):
        """
        Args:
            loess_frac: LOESS smoothing fraction (0.1 ~ 0.5 ê¶Œì¥)
            bin_size: Power binning í¬ê¸° (W), 5~30W ë²”ìœ„
            use_median: Trueë©´ ì¤‘ì•™ê°’, Falseë©´ í‰ê·  ì‚¬ìš© (deprecated, use config)
            fatmax_zone_threshold: FatMax zone ì„ê³„ê°’ (MFOì˜ ë¹„ìœ¨)
            config: AnalysisConfig ê°ì²´ (ì œê³µì‹œ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ë¬´ì‹œ)
        """
        if config:
            self.config = config
        else:
            self.config = AnalysisConfig(
                loess_frac=loess_frac,
                bin_size=max(5, min(30, bin_size)),  # 5~30W ë²”ìœ„ ì œí•œ
                aggregation_method="median" if use_median else "mean",
                fatmax_zone_threshold=fatmax_zone_threshold,
            )
        self.warnings: List[str] = []

    def analyze(self, breath_data: List[Any]) -> Optional[MetabolismAnalysisResult]:
        """
        í˜¸í¡ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì²˜ë¦¬ëœ ì‹œê³„ì—´ê³¼ ë§ˆì»¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            breath_data: BreathData ê°ì²´ ë¦¬ìŠ¤íŠ¸ (bike_power, fat_oxidation, cho_oxidation í•„ìˆ˜)

        Returns:
            MetabolismAnalysisResult ë˜ëŠ” None (ë°ì´í„° ë¶€ì¡± ì‹œ)
        """
        self.warnings = []

        # Phase trimming: êµ¬ê°„ë³„ ì œì™¸ ì˜µì…˜ ì ìš©
        filtered_data = self._apply_phase_trimming(breath_data)

        if len(filtered_data) < 10:
            self.warnings.append(
                f"Insufficient exercise data for analysis: {len(filtered_data)} points"
            )
            return None

        # === NEW: Advanced Preprocessing Pipeline ===
        # Step 0a: Gas Transport Lag Correction (shift VO2/VCO2 backward in time)
        filtered_data = self._apply_gas_transport_delay(filtered_data)

        # Step 0b: Rolling IQR Outlier Detection (remove breath-by-breath spikes)
        filtered_data = self._filter_local_outliers(filtered_data)

        # Step 0c: Recalculate Fat/CHO oxidation from cleaned VO2/VCO2 (Frayn equation)
        filtered_data = self._recalculate_oxidation_rates(filtered_data)

        # 1. Raw ë°ì´í„° ì¶”ì¶œ
        raw_points = self._extract_raw_points(filtered_data)

        if len(raw_points) < 5:
            self.warnings.append("Insufficient raw data points after extraction")
            return None

        # 2. Power Binning
        binned_points = self._power_binning(raw_points)

        if len(binned_points) < 3:
            self.warnings.append("Insufficient binned data points")
            return None

        # 3. LOESS Smoothing
        smoothed_points = self._loess_smoothing(binned_points)

        # 4. Polynomial Trend Fit (binned ë°ì´í„°ì—ì„œ ì§ì ‘ ê³„ì‚°í•˜ì—¬ ê¹”ë”í•œ í¬ë¬¼ì„  ìƒì„±)
        trend_points = self._polynomial_fit(binned_points)

        # 5. FatMax & Zone ê³„ì‚°
        fatmax_marker = self._calculate_fatmax(smoothed_points)

        # 6. Crossover Point ê³„ì‚°
        crossover_marker = self._calculate_crossover(smoothed_points)

        return MetabolismAnalysisResult(
            processed_series=ProcessedSeries(
                raw=raw_points,
                binned=binned_points,
                smoothed=smoothed_points,
                trend=trend_points,
            ),
            metabolic_markers=MetabolicMarkers(
                fat_max=fatmax_marker, crossover=crossover_marker
            ),
            warnings=self.warnings,
        )

    def _apply_phase_trimming(self, breath_data: List[Any]) -> List[Any]:
        """Phase trimming ì ìš©: Rest, Warm-up, Recovery êµ¬ê°„ ì œì™¸"""
        filtered = []

        for bd in breath_data:
            # í•„ìˆ˜ í•„ë“œ ì²´í¬
            if (
                bd.bike_power is None
                or bd.fat_oxidation is None
                or bd.cho_oxidation is None
            ):
                continue

            # Initial hyperventilation filtering (RER spike at start)
            # Exclude data points where both time < threshold AND power < threshold
            if self.config.exclude_initial_hyperventilation:
                t_sec = getattr(bd, "t_sec", None)
                if t_sec is not None and t_sec < self.config.initial_time_threshold:
                    if bd.bike_power < self.config.initial_power_threshold:
                        continue

            phase = getattr(bd, "phase", None) or ""

            # Phase ê¸°ë°˜ í•„í„°ë§
            if self.config.exclude_rest and phase.lower() in ("rest", "resting"):
                continue
            if self.config.exclude_warmup and phase.lower() in (
                "warmup",
                "warm-up",
                "warm_up",
            ):
                continue
            if self.config.exclude_recovery and phase.lower() in (
                "recovery",
                "cooldown",
                "cool-down",
            ):
                continue

            # Exercise/Peak êµ¬ê°„ë§Œ í¬í•¨ (ê¸°ë³¸)
            if phase and phase not in ("Exercise", "Peak", "exercise", "peak", ""):
                continue

            # Power threshold í•„í„°ë§
            if self.config.min_power_threshold is not None:
                if bd.bike_power < self.config.min_power_threshold:
                    continue
            if self.config.max_power_threshold is not None:
                if bd.bike_power > self.config.max_power_threshold:
                    continue

            filtered.append(bd)

        return filtered

    def _apply_gas_transport_delay(self, breath_data: List[Any]) -> List[Any]:
        """
        Gas Transport Lag Correction (ì‹œê°„ ì§€ì—° ë³´ì •)

        Physiology: Changes in muscle metabolism (Power) take ~10-20 seconds
        to be reflected in breath gas exchange (VO2/VCO2).

        Action: Shift gas metrics backward in time relative to power by
        config.gas_delay_seconds (default: 15s).
        """
        delay_sec = self.config.gas_delay_seconds
        if delay_sec <= 0 or len(breath_data) < 5:
            return breath_data

        print(f"ğŸ”¬ Applying gas transport delay: {delay_sec}s")

        # Convert to lists for processing
        times = [getattr(bd, "t_sec", i) for i, bd in enumerate(breath_data)]
        vo2_vals = [getattr(bd, "vo2", None) for bd in breath_data]
        vco2_vals = [getattr(bd, "vco2", None) for bd in breath_data]
        rer_vals = [getattr(bd, "rer", None) for bd in breath_data]
        ve_vals = [getattr(bd, "ve", None) for bd in breath_data]

        # Create interpolation functions for gas metrics
        valid_vo2 = [(t, v) for t, v in zip(times, vo2_vals) if v is not None]
        valid_vco2 = [(t, v) for t, v in zip(times, vco2_vals) if v is not None]
        valid_rer = [(t, v) for t, v in zip(times, rer_vals) if v is not None]

        if len(valid_vo2) < 5 or len(valid_vco2) < 5:
            self.warnings.append("Insufficient data for gas transport delay correction")
            return breath_data

        try:
            vo2_interp = interp1d(
                [t for t, v in valid_vo2],
                [v for t, v in valid_vo2],
                kind="linear",
                bounds_error=False,
                fill_value="extrapolate",
            )
            vco2_interp = interp1d(
                [t for t, v in valid_vco2],
                [v for t, v in valid_vco2],
                kind="linear",
                bounds_error=False,
                fill_value="extrapolate",
            )
            rer_interp = None
            if len(valid_rer) >= 5:
                rer_interp = interp1d(
                    [t for t, v in valid_rer],
                    [v for t, v in valid_rer],
                    kind="linear",
                    bounds_error=False,
                    fill_value="extrapolate",
                )

            # Apply delay: for each time point, get gas values from delay_sec earlier
            for i, bd in enumerate(breath_data):
                t = times[i]
                shifted_t = t - delay_sec

                # Update gas values with shifted interpolation
                new_vo2 = float(vo2_interp(shifted_t))
                new_vco2 = float(vco2_interp(shifted_t))

                if not (math.isnan(new_vo2) or math.isinf(new_vo2)):
                    bd.vo2 = new_vo2
                if not (math.isnan(new_vco2) or math.isinf(new_vco2)):
                    bd.vco2 = new_vco2

                if rer_interp is not None:
                    new_rer = float(rer_interp(shifted_t))
                    if not (math.isnan(new_rer) or math.isinf(new_rer)):
                        bd.rer = new_rer

        except Exception as e:
            self.warnings.append(f"Gas transport delay failed: {str(e)}")

        return breath_data

    def _filter_local_outliers(self, breath_data: List[Any]) -> List[Any]:
        """
        Rolling IQR Outlier Detection (êµ­ì†Œ ì´ìƒì¹˜ í•„í„°ë§)

        Problem: Coughs, swallows, or equipment spikes cause single-breath
        distortions that skew the average.

        Solution: Use a rolling window (default: 30s) to calculate local
        median and IQR. Replace values outside Median Â± IQR*multiplier with None.
        """
        window_sec = self.config.outlier_window_size
        iqr_mult = self.config.outlier_iqr_multiplier

        if window_sec <= 0 or len(breath_data) < 10:
            return breath_data

        print(
            f"ğŸ”¬ Filtering outliers with {window_sec}s window, IQR multiplier: {iqr_mult}"
        )

        # Get time array
        times = np.array([getattr(bd, "t_sec", i) for i, bd in enumerate(breath_data)])
        vo2_vals = np.array(
            [
                (
                    getattr(bd, "vo2", np.nan)
                    if getattr(bd, "vo2", None) is not None
                    else np.nan
                )
                for bd in breath_data
            ]
        )
        vco2_vals = np.array(
            [
                (
                    getattr(bd, "vco2", np.nan)
                    if getattr(bd, "vco2", None) is not None
                    else np.nan
                )
                for bd in breath_data
            ]
        )

        outliers_removed = 0

        for i in range(len(breath_data)):
            t = times[i]
            # Find indices within window
            mask = (times >= t - window_sec / 2) & (times <= t + window_sec / 2)

            # Check VO2
            local_vo2 = vo2_vals[mask]
            local_vo2 = local_vo2[~np.isnan(local_vo2)]
            if len(local_vo2) >= 5 and not np.isnan(vo2_vals[i]):
                q1, median, q3 = np.percentile(local_vo2, [25, 50, 75])
                iqr = q3 - q1
                lower = median - iqr_mult * iqr
                upper = median + iqr_mult * iqr
                if vo2_vals[i] < lower or vo2_vals[i] > upper:
                    breath_data[i].vo2 = None
                    outliers_removed += 1

            # Check VCO2
            local_vco2 = vco2_vals[mask]
            local_vco2 = local_vco2[~np.isnan(local_vco2)]
            if len(local_vco2) >= 5 and not np.isnan(vco2_vals[i]):
                q1, median, q3 = np.percentile(local_vco2, [25, 50, 75])
                iqr = q3 - q1
                lower = median - iqr_mult * iqr
                upper = median + iqr_mult * iqr
                if vco2_vals[i] < lower or vco2_vals[i] > upper:
                    breath_data[i].vco2 = None
                    outliers_removed += 1

        if outliers_removed > 0:
            print(f"ğŸ”¬ Filtered {outliers_removed} outlier values using rolling IQR")

        return breath_data

    def _recalculate_oxidation_rates(self, breath_data: List[Any]) -> List[Any]:
        """
        Recalculate Fat/CHO Oxidation Rates using Frayn Equation

        Critical Step: After gas delay and outlier filtering, VO2/VCO2 values
        have been modified. Fat/CHO oxidation must be recalculated to reflect
        these cleaned values.

        Frayn Equations (input VO2/VCO2 in L/min):
        - Fat (g/min) = 1.67 * VO2 - 1.67 * VCO2
        - CHO (g/min) = 4.55 * VCO2 - 3.21 * VO2

        Note: Input VO2/VCO2 are typically in mL/min, so divide by 1000.
        """
        if len(breath_data) < 5:
            return breath_data

        print(f"ğŸ”¬ Recalculating Fat/CHO oxidation rates from cleaned VO2/VCO2...")

        recalculated = 0
        set_to_none = 0

        for bd in breath_data:
            vo2 = getattr(bd, "vo2", None)
            vco2 = getattr(bd, "vco2", None)

            # If VO2 or VCO2 were filtered out (set to None), set oxidation to None too
            if vo2 is None or vco2 is None:
                bd.fat_oxidation = None
                bd.cho_oxidation = None
                set_to_none += 1
                continue

            try:
                # Convert mL/min to L/min
                vo2_l = vo2 / 1000.0
                vco2_l = vco2 / 1000.0

                # Frayn Equations
                fat_ox = 1.67 * vo2_l - 1.67 * vco2_l
                cho_ox = 4.55 * vco2_l - 3.21 * vo2_l

                # Non-negative constraint (physiologically, can't have negative oxidation)
                if self.config.non_negative_constraint:
                    fat_ox = max(0.0, fat_ox)
                    cho_ox = max(0.0, cho_ox)

                bd.fat_oxidation = fat_ox
                bd.cho_oxidation = cho_ox
                recalculated += 1

            except (TypeError, ValueError) as e:
                bd.fat_oxidation = None
                bd.cho_oxidation = None
                set_to_none += 1

        print(
            f"ğŸ”¬ Recalculated {recalculated} points, {set_to_none} points set to None (filtered)"
        )

        return breath_data

    def _extract_raw_points(self, breath_data: List[Any]) -> List[ProcessedDataPoint]:
        """í˜¸í¡ ë°ì´í„°ì—ì„œ raw í¬ì¸íŠ¸ ì¶”ì¶œ"""

        # Helper to safely convert to float (handles None, NaN, and 0 correctly)
        def safe_float(val):
            if val is None:
                return None
            try:
                f = float(val)
                if math.isnan(f) or math.isinf(f):
                    return None
                return f
            except (ValueError, TypeError):
                return None

        points = []
        for bd in breath_data:
            points.append(
                ProcessedDataPoint(
                    power=float(bd.bike_power) if bd.bike_power is not None else 0.0,
                    fat_oxidation=safe_float(bd.fat_oxidation),
                    cho_oxidation=safe_float(bd.cho_oxidation),
                    rer=safe_float(bd.rer),
                    vo2=safe_float(getattr(bd, "vo2", None)),
                    vco2=safe_float(getattr(bd, "vco2", None)),
                    hr=safe_float(getattr(bd, "hr", None)),
                    ve_vo2=safe_float(getattr(bd, "ve_vo2", None)),
                    ve_vco2=safe_float(getattr(bd, "ve_vco2", None)),
                    count=1,
                )
            )
        return points

    def _power_binning(
        self, raw_points: List[ProcessedDataPoint]
    ) -> List[ProcessedDataPoint]:
        """
        Power ë°ì´í„°ë¥¼ bin_size W ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”í•˜ê³  ì¤‘ì•™ê°’/í‰ê·  ê³„ì‚°

        Args:
            raw_points: Raw ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            Binned ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ (power ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬)
        """
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(
            [
                {
                    "power": p.power,
                    "fat_oxidation": p.fat_oxidation,
                    "cho_oxidation": p.cho_oxidation,
                    "rer": p.rer,
                    "vo2": p.vo2,
                    "vco2": p.vco2,
                    "hr": p.hr,
                    "ve_vo2": p.ve_vo2,
                    "ve_vco2": p.ve_vco2,
                }
                for p in raw_points
            ]
        )

        # Power bin í• ë‹¹
        bin_size = self.config.bin_size
        df["power_bin"] = (df["power"] / bin_size).round() * bin_size

        # ì§‘ê³„í•  í•„ë“œ ëª©ë¡
        numeric_fields = [
            "fat_oxidation",
            "cho_oxidation",
            "rer",
            "vo2",
            "vco2",
            "hr",
            "ve_vo2",
            "ve_vco2",
        ]

        # ì§‘ê³„ ë°©ë²•ì— ë”°ë¥¸ ê·¸ë£¹í™”
        agg_method = self.config.aggregation_method

        if agg_method == "median":
            agg_dict = {field: "median" for field in numeric_fields}
            agg_dict["power"] = "count"
            agg_df = df.groupby("power_bin").agg(agg_dict).reset_index()
        elif agg_method == "mean":
            agg_dict = {field: "mean" for field in numeric_fields}
            agg_dict["power"] = "count"
            agg_df = df.groupby("power_bin").agg(agg_dict).reset_index()
        elif agg_method == "trimmed_mean":
            # Trimmed mean: ì–‘ìª½ 10% ì œê±° í›„ í‰ê· 
            proportiontocut = self.config.trimmed_mean_proportiontocut

            def safe_trim_mean(x):
                if len(x) < 4:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¼ë°˜ í‰ê· 
                    return x.mean()
                return trim_mean(x, proportiontocut)

            agg_dict = {field: safe_trim_mean for field in numeric_fields}
            agg_dict["power"] = "count"
            agg_df = df.groupby("power_bin").agg(agg_dict).reset_index()
        else:
            # ê¸°ë³¸ê°’: median
            agg_dict = {field: "median" for field in numeric_fields}
            agg_dict["power"] = "count"
            agg_df = df.groupby("power_bin").agg(agg_dict).reset_index()

        agg_df = agg_df.rename(columns={"power": "count"})
        agg_df = agg_df.sort_values("power_bin").reset_index(drop=True)

        # ê²°ê³¼ ë³€í™˜
        binned_points = []
        for _, row in agg_df.iterrows():
            fat_ox = (
                float(row["fat_oxidation"]) if pd.notna(row["fat_oxidation"]) else None
            )
            cho_ox = (
                float(row["cho_oxidation"]) if pd.notna(row["cho_oxidation"]) else None
            )
            rer_val = float(row["rer"]) if pd.notna(row["rer"]) else None
            vo2_val = float(row["vo2"]) if pd.notna(row["vo2"]) else None
            vco2_val = float(row["vco2"]) if pd.notna(row["vco2"]) else None
            hr_val = float(row["hr"]) if pd.notna(row["hr"]) else None
            ve_vo2_val = float(row["ve_vo2"]) if pd.notna(row["ve_vo2"]) else None
            ve_vco2_val = float(row["ve_vco2"]) if pd.notna(row["ve_vco2"]) else None

            # Non-negative constraint (for oxidation values only)
            if self.config.non_negative_constraint:
                if fat_ox is not None:
                    fat_ox = max(0.0, fat_ox)
                if cho_ox is not None:
                    cho_ox = max(0.0, cho_ox)

            binned_points.append(
                ProcessedDataPoint(
                    power=float(row["power_bin"]),
                    fat_oxidation=fat_ox,
                    cho_oxidation=cho_ox,
                    rer=rer_val,
                    vo2=vo2_val,
                    vco2=vco2_val,
                    hr=hr_val,
                    ve_vo2=ve_vo2_val,
                    ve_vco2=ve_vco2_val,
                    count=int(row["count"]),
                )
            )

        return binned_points

    def _loess_smoothing(
        self, binned_points: List[ProcessedDataPoint]
    ) -> List[ProcessedDataPoint]:
        """
        LOESS (Locally Estimated Scatterplot Smoothing) ì ìš©

        Args:
            binned_points: Binned ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            Smoothed ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        if not HAS_STATSMODELS:
            self.warnings.append(
                "statsmodels not available, using binned data as smoothed"
            )
            return binned_points

        if len(binned_points) < 4:
            self.warnings.append(
                "Not enough data points for LOESS smoothing, using binned data"
            )
            return binned_points

        # ë°ì´í„° ì¶”ì¶œ
        powers = np.array([p.power for p in binned_points])
        fat_ox = np.array(
            [
                p.fat_oxidation if p.fat_oxidation is not None else 0
                for p in binned_points
            ]
        )
        cho_ox = np.array(
            [
                p.cho_oxidation if p.cho_oxidation is not None else 0
                for p in binned_points
            ]
        )
        rer_vals = np.array(
            [p.rer if p.rer is not None else np.nan for p in binned_points]
        )
        vo2_vals = np.array(
            [p.vo2 if p.vo2 is not None else np.nan for p in binned_points]
        )
        vco2_vals = np.array(
            [p.vco2 if p.vco2 is not None else np.nan for p in binned_points]
        )
        hr_vals = np.array(
            [p.hr if p.hr is not None else np.nan for p in binned_points]
        )
        ve_vo2_vals = np.array(
            [p.ve_vo2 if p.ve_vo2 is not None else np.nan for p in binned_points]
        )
        ve_vco2_vals = np.array(
            [p.ve_vco2 if p.ve_vco2 is not None else np.nan for p in binned_points]
        )

        # LOESS smoothing
        try:
            # frac ê°’ ì¡°ì • (ë°ì´í„° í¬ì¸íŠ¸ê°€ ì ì„ ê²½ìš°)
            frac = min(self.config.loess_frac, (len(powers) - 1) / len(powers))
            frac = max(frac, 0.15)  # ìµœì†Œ 0.15

            fat_smoothed = lowess(fat_ox, powers, frac=frac, return_sorted=True)
            cho_smoothed = lowess(cho_ox, powers, frac=frac, return_sorted=True)

            # Helper function for optional LOESS smoothing
            def smooth_optional(vals, powers, frac, min_points=4):
                if np.all(np.isnan(vals)):
                    return None
                valid_idx = ~np.isnan(vals)
                if np.sum(valid_idx) >= min_points:
                    return lowess(
                        vals[valid_idx],
                        powers[valid_idx],
                        frac=frac,
                        return_sorted=True,
                    )
                return None

            rer_smoothed = smooth_optional(rer_vals, powers, frac)
            vo2_smoothed = smooth_optional(vo2_vals, powers, frac)
            vco2_smoothed = smooth_optional(vco2_vals, powers, frac)
            hr_smoothed = smooth_optional(hr_vals, powers, frac)
            ve_vo2_smoothed = smooth_optional(ve_vo2_vals, powers, frac)
            ve_vco2_smoothed = smooth_optional(ve_vco2_vals, powers, frac)

            # Helper to interpolate smoothed values
            def get_interpolated_val(smoothed, power_val, constraint=None):
                if smoothed is None:
                    return None
                idx = np.argmin(np.abs(smoothed[:, 0] - power_val))
                val = float(smoothed[idx, 1])
                if constraint and not (constraint[0] <= val <= constraint[1]):
                    return None
                return val

            # ê²°ê³¼ ìƒì„±
            smoothed_points = []
            for i in range(len(fat_smoothed)):
                power_val = fat_smoothed[i, 0]
                fat_val = float(fat_smoothed[i, 1])
                cho_val = float(cho_smoothed[i, 1])

                # NaN ì²´í¬ ë° ì²˜ë¦¬
                if math.isnan(fat_val) or math.isinf(fat_val):
                    fat_val = 0.0
                if math.isnan(cho_val) or math.isinf(cho_val):
                    cho_val = 0.0

                # Non-negative constraint for oxidation values
                if self.config.non_negative_constraint:
                    fat_val = max(0.0, fat_val)
                    cho_val = max(0.0, cho_val)

                smoothed_points.append(
                    ProcessedDataPoint(
                        power=float(power_val),
                        fat_oxidation=fat_val,
                        cho_oxidation=cho_val,
                        rer=get_interpolated_val(rer_smoothed, power_val, (0.5, 1.5)),
                        vo2=get_interpolated_val(vo2_smoothed, power_val),
                        vco2=get_interpolated_val(vco2_smoothed, power_val),
                        hr=get_interpolated_val(hr_smoothed, power_val),
                        ve_vo2=get_interpolated_val(ve_vo2_smoothed, power_val),
                        ve_vco2=get_interpolated_val(ve_vco2_smoothed, power_val),
                        count=None,
                    )
                )

            return smoothed_points

        except Exception as e:
            self.warnings.append(f"LOESS smoothing failed: {str(e)}, using binned data")
            return binned_points

    def _polynomial_fit(
        self, binned_points: List[ProcessedDataPoint]
    ) -> List[ProcessedDataPoint]:
        """
        Polynomial Regressionì„ ì‚¬ìš©í•œ Trend Line ê³„ì‚° (ê¹”ë”í•œ ê³¡ì„  ìƒì„±)

        Polynomial Degrees by Metric Type (physiological standard models):
        - Fat & CHO (FatMax): Degree 3 - inverted U-shape / J-curve
        - RER: Degree 3 - slight dip at start, exponential rise at end
        - VO2, VCO2: Degree 2 - linear efficiency (slight curve)
        - HR: Degree 2 - linear response
        - VT Equivalents (VE/VO2, VE/VCO2): Degree 2 - U-shape for nadir detection

        Note: binned ë°ì´í„°ì—ì„œ ì§ì ‘ ê³„ì‚°í•˜ì—¬ smooth ë°ì´í„°ë³´ë‹¤ ë” ë‹¨ìˆœí•œ íŒ¨í„´ ìƒì„±

        Args:
            binned_points: Binned ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            Trend ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ (10W ê°„ê²©)
        """
        if len(binned_points) < 4:
            self.warnings.append("Not enough data points for polynomial fit")
            print(f"âš ï¸ Polynomial fit skipped: only {len(binned_points)} points")
            return []

        try:
            print(f"ğŸ”¬ Starting polynomial fit with {len(binned_points)} binned points")

            # ë°ì´í„° ì¶”ì¶œ
            powers = np.array([p.power for p in binned_points])
            fat_ox = np.array(
                [
                    p.fat_oxidation if p.fat_oxidation is not None else 0
                    for p in binned_points
                ]
            )
            cho_ox = np.array(
                [
                    p.cho_oxidation if p.cho_oxidation is not None else 0
                    for p in binned_points
                ]
            )
            rer_vals = np.array(
                [p.rer if p.rer is not None else np.nan for p in binned_points]
            )
            vo2_vals = np.array(
                [p.vo2 if p.vo2 is not None else np.nan for p in binned_points]
            )
            vco2_vals = np.array(
                [p.vco2 if p.vco2 is not None else np.nan for p in binned_points]
            )
            hr_vals = np.array(
                [p.hr if p.hr is not None else np.nan for p in binned_points]
            )
            ve_vo2_vals = np.array(
                [p.ve_vo2 if p.ve_vo2 is not None else np.nan for p in binned_points]
            )
            ve_vco2_vals = np.array(
                [p.ve_vco2 if p.ve_vco2 is not None else np.nan for p in binned_points]
            )

            # Polynomial degrees per metric type
            DEGREE_FAT_CHO = 3  # Inverted U-shape for Fat, J-curve for CHO
            DEGREE_RER = 3  # Slight dip at start, exponential rise at end
            DEGREE_VO2_VCO2 = 2  # Linear efficiency (slight curve)
            DEGREE_HR = 2  # Linear response
            DEGREE_VT = 2  # U-shape for nadir detection

            # Helper function for fitting polynomial with NaN handling
            def fit_poly(vals, powers, degree, min_points=4):
                if np.all(np.isnan(vals)):
                    return None
                valid_idx = ~np.isnan(vals)
                if np.sum(valid_idx) >= min_points:
                    # Limit degree if not enough points
                    effective_degree = min(degree, np.sum(valid_idx) - 1)
                    coeffs = np.polyfit(
                        powers[valid_idx], vals[valid_idx], effective_degree
                    )
                    return np.poly1d(coeffs)
                return None

            # Fit polynomials
            fat_poly = np.poly1d(np.polyfit(powers, fat_ox, DEGREE_FAT_CHO))
            cho_poly = np.poly1d(np.polyfit(powers, cho_ox, DEGREE_FAT_CHO))
            rer_poly = fit_poly(rer_vals, powers, DEGREE_RER)
            vo2_poly = fit_poly(vo2_vals, powers, DEGREE_VO2_VCO2)
            vco2_poly = fit_poly(vco2_vals, powers, DEGREE_VO2_VCO2)
            hr_poly = fit_poly(hr_vals, powers, DEGREE_HR)
            ve_vo2_poly = fit_poly(ve_vo2_vals, powers, DEGREE_VT)
            ve_vco2_poly = fit_poly(ve_vco2_vals, powers, DEGREE_VT)

            # Trend í¬ì¸íŠ¸ ìƒì„± (10W ê°„ê²©) - with Sparse Data Handling
            power_min = int(np.floor(powers.min() / 10) * 10)
            power_max = int(np.ceil(powers.max() / 10) * 10)
            trend_powers = np.arange(power_min, power_max + 1, 10)

            # === Sparse Data Handling: Detect gaps in binned data ===
            gap_threshold = self.config.trend_gap_threshold_watts
            sorted_powers = np.sort(powers)
            gaps = []
            for i in range(1, len(sorted_powers)):
                gap_size = sorted_powers[i] - sorted_powers[i - 1]
                if gap_size > gap_threshold:
                    gaps.append((sorted_powers[i - 1], sorted_powers[i]))

            if gaps:
                print(f"ğŸ”¬ Detected {len(gaps)} data gap(s) > {gap_threshold}W: {gaps}")

            def is_in_gap(p):
                """Check if power value falls within a data gap."""
                for gap_start, gap_end in gaps:
                    if gap_start < p < gap_end:
                        return True
                return False

            # Helper to check if enough data exists near this power
            def has_nearby_data(p, threshold=None):
                """Check if binned data exists within threshold of power p."""
                if threshold is None:
                    threshold = gap_threshold / 2
                return np.any(np.abs(sorted_powers - p) <= threshold)

            # Helper to safely evaluate polynomial
            def eval_poly(poly, p, constraint=None, default=None):
                if poly is None:
                    return default
                val = float(poly(p))
                if math.isnan(val) or math.isinf(val):
                    return default
                if constraint and not (constraint[0] <= val <= constraint[1]):
                    return default
                return val

            trend_points = []
            skipped_count = 0
            for p in trend_powers:
                # Skip trend points in data gaps (sparse regions)
                if is_in_gap(p):
                    skipped_count += 1
                    continue

                # Skip trend points too far from any binned data
                if not has_nearby_data(p):
                    skipped_count += 1
                    continue

                fat_val = eval_poly(fat_poly, p, default=0.0)
                cho_val = eval_poly(cho_poly, p, default=0.0)

                # Non-negative constraint for oxidation values
                if self.config.non_negative_constraint:
                    fat_val = max(0.0, fat_val)
                    cho_val = max(0.0, cho_val)

                trend_points.append(
                    ProcessedDataPoint(
                        power=float(p),
                        fat_oxidation=fat_val,
                        cho_oxidation=cho_val,
                        rer=eval_poly(rer_poly, p, constraint=(0.5, 1.5)),
                        vo2=eval_poly(vo2_poly, p),
                        vco2=eval_poly(vco2_poly, p),
                        hr=eval_poly(hr_poly, p),
                        ve_vo2=eval_poly(ve_vo2_poly, p),
                        ve_vco2=eval_poly(ve_vco2_poly, p),
                        count=None,
                    )
                )

            if skipped_count > 0:
                print(f"ğŸ”¬ Skipped {skipped_count} trend points in sparse regions")

            print(
                f"âœ… Polynomial fit complete: {len(trend_points)} trend points generated"
            )
            return trend_points

        except Exception as e:
            self.warnings.append(f"Polynomial fit failed: {str(e)}")
            print(f"âŒ Polynomial fit failed: {str(e)}")
            return []

    def _calculate_fatmax(
        self, smoothed_points: List[ProcessedDataPoint]
    ) -> FatMaxMarker:
        """
        FatMax (Maximum Fat Oxidation) ë° Zone ê³„ì‚°

        Args:
            smoothed_points: Smoothed ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            FatMaxMarker
        """
        if not smoothed_points:
            return FatMaxMarker(power=0, mfo=0, zone_min=0, zone_max=0)

        # MFO (Maximum Fat Oxidation) ì°¾ê¸°
        max_fat = 0.0
        max_fat_power = 0.0
        for p in smoothed_points:
            if p.fat_oxidation is not None and p.fat_oxidation > max_fat:
                max_fat = p.fat_oxidation
                max_fat_power = p.power

        if max_fat == 0:
            return FatMaxMarker(power=0, mfo=0, zone_min=0, zone_max=0)

        # FatMax Zone ê³„ì‚° (MFOì˜ 90% ì´ìƒ ìœ ì§€ êµ¬ê°„)
        threshold = max_fat * self.config.fatmax_zone_threshold
        zone_powers = [
            p.power
            for p in smoothed_points
            if p.fat_oxidation is not None and p.fat_oxidation >= threshold
        ]

        if zone_powers:
            zone_min = min(zone_powers)
            zone_max = max(zone_powers)
        else:
            zone_min = max_fat_power
            zone_max = max_fat_power

        return FatMaxMarker(
            power=int(round(max_fat_power)),
            mfo=max_fat,
            zone_min=int(round(zone_min)),
            zone_max=int(round(zone_max)),
        )

    def _calculate_crossover(
        self, smoothed_points: List[ProcessedDataPoint]
    ) -> CrossoverMarker:
        """
        Crossover Point (FatOx = CHOOx ì§€ì ) ê³„ì‚°

        Args:
            smoothed_points: Smoothed ë°ì´í„° í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            CrossoverMarker
        """
        if len(smoothed_points) < 3:
            return CrossoverMarker(power=None, fat_value=None, cho_value=None)

        # ë°ì´í„° ì¶”ì¶œ
        powers = np.array([p.power for p in smoothed_points])
        fat_ox = np.array(
            [
                p.fat_oxidation if p.fat_oxidation is not None else 0
                for p in smoothed_points
            ]
        )
        cho_ox = np.array(
            [
                p.cho_oxidation if p.cho_oxidation is not None else 0
                for p in smoothed_points
            ]
        )

        # FatOx - CHOOx ì°¨ì´
        diff = fat_ox - cho_ox

        # ë¶€í˜¸ ë³€í™” ì§€ì  ì°¾ê¸° (ì–‘ â†’ ìŒ: Fat > CHO ì—ì„œ Fat < CHOë¡œ)
        sign_changes = []
        for i in range(len(diff) - 1):
            if diff[i] > 0 and diff[i + 1] <= 0:
                sign_changes.append(i)
            elif diff[i] >= 0 and diff[i + 1] < 0:
                sign_changes.append(i)

        if not sign_changes:
            # êµì°¨ì  ì—†ìŒ
            return CrossoverMarker(power=None, fat_value=None, cho_value=None)

        try:
            # ì²« ë²ˆì§¸ êµì°¨ì  ì‚¬ìš© (ì¼ë°˜ì ìœ¼ë¡œ ìš´ë™ ê°•ë„ê°€ ì¦ê°€í•˜ë©´ì„œ ì²« êµì°¨)
            idx = sign_changes[0]

            # ì„ í˜• ë³´ê°„ìœ¼ë¡œ ì •í™•í•œ êµì°¨ì  ê³„ì‚°
            # diff[idx] > 0, diff[idx+1] <= 0
            p1, p2 = powers[idx], powers[idx + 1]
            d1, d2 = diff[idx], diff[idx + 1]

            if d1 == d2:
                crossover_power = p1
            else:
                # ì„ í˜• ë³´ê°„: d1 + (d2 - d1) * t = 0 => t = -d1 / (d2 - d1)
                t = -d1 / (d2 - d1)
                crossover_power = p1 + t * (p2 - p1)

            # êµì°¨ì ì—ì„œì˜ Fat/CHO ê°’ ê³„ì‚° (ì„ í˜• ë³´ê°„)
            crossover_fat = fat_ox[idx] + t * (fat_ox[idx + 1] - fat_ox[idx])
            crossover_cho = cho_ox[idx] + t * (cho_ox[idx + 1] - cho_ox[idx])

            return CrossoverMarker(
                power=int(round(crossover_power)),
                fat_value=float(crossover_fat),
                cho_value=float(crossover_cho),
            )

        except Exception as e:
            self.warnings.append(f"Crossover calculation failed: {str(e)}")
            return CrossoverMarker(power=None, fat_value=None, cho_value=None)


def analyze_metabolism(
    breath_data: List[Any],
    loess_frac: float = 0.25,
    bin_size: int = 10,
    use_median: bool = True,
    aggregation_method: Optional[str] = None,
    exclude_rest: bool = True,
    exclude_warmup: bool = True,
    exclude_recovery: bool = True,
    min_power_threshold: Optional[int] = None,
    fatmax_zone_threshold: float = 0.90,
) -> Optional[MetabolismAnalysisResult]:
    """
    í˜¸í¡ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì²˜ë¦¬ëœ ì‹œê³„ì—´ê³¼ ë§ˆì»¤ë¥¼ ë°˜í™˜í•˜ëŠ” í¸ì˜ í•¨ìˆ˜

    Args:
        breath_data: BreathData ê°ì²´ ë¦¬ìŠ¤íŠ¸
        loess_frac: LOESS smoothing fraction (0.1~0.5, default: 0.25)
        bin_size: Power binning í¬ê¸° (W, default: 10)
        use_median: Trueë©´ ì¤‘ì•™ê°’, Falseë©´ í‰ê·  ì‚¬ìš© (deprecated)
        aggregation_method: ì§‘ê³„ ë°©ë²• ("median", "mean", "trimmed_mean")
        exclude_rest: Rest êµ¬ê°„ ì œì™¸ ì—¬ë¶€
        exclude_warmup: Warm-up êµ¬ê°„ ì œì™¸ ì—¬ë¶€
        exclude_recovery: Recovery êµ¬ê°„ ì œì™¸ ì—¬ë¶€
        min_power_threshold: ìµœì†Œ íŒŒì›Œ ì„ê³„ê°’ (e.g., 50W ë¯¸ë§Œ ì œì™¸)
        fatmax_zone_threshold: FatMax zone ì„ê³„ê°’ (MFOì˜ ë¹„ìœ¨)

    Returns:
        MetabolismAnalysisResult ë˜ëŠ” None
    """
    # aggregation_methodê°€ ì œê³µë˜ë©´ ìš°ì„ , ì•„ë‹ˆë©´ use_median ê¸°ë°˜
    if aggregation_method is None:
        aggregation_method = "median" if use_median else "mean"

    config = AnalysisConfig(
        loess_frac=loess_frac,
        bin_size=bin_size,
        aggregation_method=aggregation_method,
        fatmax_zone_threshold=fatmax_zone_threshold,
        exclude_rest=exclude_rest,
        exclude_warmup=exclude_warmup,
        exclude_recovery=exclude_recovery,
        min_power_threshold=min_power_threshold,
    )

    analyzer = MetabolismAnalyzer(config=config)
    return analyzer.analyze(breath_data)
