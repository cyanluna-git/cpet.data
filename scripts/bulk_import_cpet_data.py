"""
CPET_data í´ë”ì˜ ëª¨ë“  Excel íŒŒì¼ì„ DBì— ì¼ê´„ ì—…ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/bulk_import_cpet_data.py [--dry-run] [--limit N]
    
ì˜µì…˜:
    --dry-run: ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ íŒŒì¼ ëª©ë¡ë§Œ ì¶œë ¥
    --limit N: ì²˜ìŒ Nê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
"""

import argparse
import asyncio
import os
import sys
import re
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, Dict, Any
import json

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

import httpx
import pandas as pd


# Configuration
API_BASE_URL = "http://localhost:8100/api"
CPET_DATA_DIR = Path(__file__).parent.parent / "CPET_data"

# Auth credentials
ADMIN_EMAIL = "gerald.park@cpet.com"
ADMIN_PASSWORD = "cpet2026!"


def extract_subject_from_excel(file_path: Path) -> Dict[str, Any]:
    """
    Excel íŒŒì¼ ë‚´ë¶€ì—ì„œ í”¼í—˜ì ì •ë³´ ì¶”ì¶œ (ID1 ì„¹ì…˜)
    
    Returns:
        {
            'last_name': str,
            'first_name': str,
            'gender': str,
            'age': float,
            'height_cm': float,
            'weight_kg': float,
            'birth_date': str
        }
    """
    try:
        df = pd.read_excel(file_path, header=None, nrows=20)
        
        def safe_get(row, col):
            try:
                val = df.iloc[row, col]
                if pd.isna(val):
                    return None
                return val
            except:
                return None
        
        return {
            'last_name': str(safe_get(1, 1) or '').strip(),
            'first_name': str(safe_get(2, 1) or '').strip(),
            'gender': str(safe_get(3, 1) or '').strip(),
            'age': safe_get(4, 1),
            'height_cm': safe_get(5, 1),
            'weight_kg': safe_get(6, 1),
            'birth_date': str(safe_get(7, 1) or '').strip(),
        }
    except Exception as e:
        print(f"  âš ï¸ Excel íŒŒì‹± ì—ëŸ¬: {e}")
        return {}


def extract_subject_info(filename: str) -> Tuple[str, str, str, Optional[datetime]]:
    """
    íŒŒì¼ëª…ì—ì„œ í”¼í—˜ì ì •ë³´ ì¶”ì¶œ
    
    í˜•ì‹: "LastName FirstName YYYYMMDD CPET TYPE_timestamp.xlsx"
    
    Returns:
        (last_name, first_name, research_id, test_date)
    """
    parts = filename.replace('.xlsx', '').replace('.xls', '').split(' ')
    
    if len(parts) < 3:
        return None, None, None, None
    
    last_name = parts[0].strip()
    first_name = parts[1].strip()
    
    # ì´ë¦„ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì œê±° (ì˜ˆ: Haesung20240403 -> Haesung)
    first_name = re.sub(r'\d{8}$', '', first_name)
    
    # ì •ê·œí™”
    last_name = last_name.capitalize()
    first_name = first_name.capitalize()
    
    # Research ID ìƒì„±
    research_id = f"SUB-{last_name.upper()[:3]}-{first_name.upper()[:3]}"
    
    # ë‚ ì§œ íŒŒì‹±
    test_date = None
    for part in parts:
        if re.match(r'^\d{8}$', part):
            try:
                test_date = datetime.strptime(part, '%Y%m%d')
            except ValueError:
                pass
            break
    
    return last_name, first_name, research_id, test_date


async def get_auth_token(client: httpx.AsyncClient) -> Optional[str]:
    """ë¡œê·¸ì¸í•˜ì—¬ JWT í† í° íšë“"""
    try:
        response = await client.post(
            f"{API_BASE_URL}/auth/login",
            data={
                "username": ADMIN_EMAIL,
                "password": ADMIN_PASSWORD,
            },
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        if response.status_code == 200:
            data = response.json()
            return data.get("access_token")
        else:
            print(f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"âŒ ë¡œê·¸ì¸ ì—ëŸ¬: {e}")
        return None


async def get_subject_by_name(
    client: httpx.AsyncClient,
    token: str,
    last_name: str,
    first_name: str,
    cache: dict = None
) -> Optional[str]:
    """ì´ë¦„ìœ¼ë¡œ subject_id ì¡°íšŒ (ìºì‹± ì§€ì›)"""
    cache_key = f"{last_name}_{first_name}".lower()
    
    # ìºì‹œ í™•ì¸
    if cache and cache_key in cache:
        return cache[cache_key]
    
    for attempt in range(3):  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        try:
            # ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
            response = await client.get(
                f"{API_BASE_URL}/subjects",
                params={"search": last_name, "page_size": 100},
                headers={"Authorization": f"Bearer {token}"},
                timeout=30.0
            )
            
            if response.status_code == 200:
                data = response.json()
                for subject in data.get("items", []):
                    # encrypted_name í˜•ì‹: "FirstName LastName"
                    subject_name = subject.get("encrypted_name", "").lower()
                    if (last_name.lower() in subject_name and 
                        first_name.lower() in subject_name):
                        subject_id = subject.get("id")
                        if cache is not None:
                            cache[cache_key] = subject_id
                        return subject_id
            return None
        except Exception as e:
            if attempt < 2:
                await asyncio.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                continue
            print(f"  âš ï¸ Subject ì¡°íšŒ ì—ëŸ¬: {e}")
            return None


async def get_subject_id(
    client: httpx.AsyncClient,
    token: str,
    research_id: str,
    cache: dict = None
) -> Optional[str]:
    """research_idë¡œ subject_id ì¡°íšŒ (ìºì‹± ì§€ì›) - deprecated, use get_subject_by_name"""
    # ìºì‹œ í™•ì¸
    if cache and research_id in cache:
        return cache[research_id]
    
    for attempt in range(3):  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        try:
            response = await client.get(
                f"{API_BASE_URL}/subjects",
                params={"search": research_id, "page_size": 100},
                headers={"Authorization": f"Bearer {token}"},
                timeout=30.0
            )
            
            if response.status_code == 200:
                data = response.json()
                for subject in data.get("items", []):
                    if subject.get("research_id") == research_id:
                        subject_id = subject.get("id")
                        if cache is not None:
                            cache[research_id] = subject_id
                        return subject_id
            return None
        except Exception as e:
            if attempt < 2:
                await asyncio.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                continue
            print(f"  âš ï¸ Subject ì¡°íšŒ ì—ëŸ¬: {e}")
            return None


async def upload_file_once(
    client: httpx.AsyncClient,
    token: str,
    file_path: Path,
    subject_id: str,
    calc_method: str = "Frayn",
    smoothing_window: int = 10
) -> Tuple[bool, Optional[str], Optional[dict]]:
    """
    ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œ ì‹œë„
    """
    try:
        with open(file_path, "rb") as f:
            files = {"file": (file_path.name, f, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")}
            data = {
                "subject_id": subject_id,
                "calc_method": calc_method,
                "smoothing_window": str(smoothing_window),
            }
            
            response = await client.post(
                f"{API_BASE_URL}/tests/upload",
                files=files,
                data=data,
                headers={"Authorization": f"Bearer {token}"},
                timeout=180.0  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            )
        
        if response.status_code in (200, 201):
            result = response.json()
            test_id = result.get("test_id") or result.get("test", {}).get("test_id")
            return True, test_id, result
        else:
            return False, None, {"status": response.status_code, "detail": response.text}
            
    except Exception as e:
        return False, None, {"error": str(e)}


async def upload_file_with_retry(
    token: str,
    file_path: Path,
    subject_id: str,
    calc_method: str = "Frayn",
    smoothing_window: int = 10,
    max_retries: int = 3,
    retry_delay: float = 5.0
) -> Tuple[bool, Optional[str], Optional[dict]]:
    """
    íŒŒì¼ ì—…ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    ë§¤ë²ˆ ìƒˆ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ì—°ê²° ë¬¸ì œ ë°©ì§€
    """
    last_error = None
    
    for attempt in range(max_retries):
        if attempt > 0:
            print(f"      â†» ì¬ì‹œë„ {attempt + 1}/{max_retries} (ëŒ€ê¸° {retry_delay}ì´ˆ)")
            await asyncio.sleep(retry_delay)
        
        # ìƒˆ í´ë¼ì´ì–¸íŠ¸ë¡œ ì—°ê²° (ì—°ê²° í’€ ë¬¸ì œ ë°©ì§€)
        async with httpx.AsyncClient() as client:
            success, test_id, info = await upload_file_once(
                client, token, file_path, subject_id, calc_method, smoothing_window
            )
            
            if success:
                return success, test_id, info
            
            last_error = info
            
            # 500 ì—ëŸ¬ëŠ” ì¬ì‹œë„
            if isinstance(info, dict) and info.get("status") == 500:
                print(f"      âš ï¸ ì„œë²„ ì—ëŸ¬ (500), ì¬ì‹œë„...")
                continue
            
            # íƒ€ì„ì•„ì›ƒì€ ì¬ì‹œë„
            if isinstance(info, dict) and "timeout" in str(info.get("error", "")).lower():
                print(f"      âš ï¸ íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„...")
                continue
            
            # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì‹¤íŒ¨
            break
    
    return False, None, last_error


def collect_excel_files(data_dir: Path) -> list[Path]:
    """CPET_data í´ë”ì—ì„œ ëª¨ë“  Excel íŒŒì¼ ìˆ˜ì§‘"""
    files = []
    
    for root, dirs, filenames in os.walk(data_dir):
        for filename in filenames:
            if filename.endswith(('.xlsx', '.xls')) and not filename.startswith('~$'):
                files.append(Path(root) / filename)
    
    # ë‚ ì§œìˆœ ì •ë ¬
    files.sort(key=lambda p: p.name)
    return files


async def main():
    parser = argparse.ArgumentParser(description="CPET ë°ì´í„° ì¼ê´„ ì„í¬íŠ¸")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ íŒŒì¼ ëª©ë¡ë§Œ ì¶œë ¥")
    parser.add_argument("--limit", type=int, default=0, help="ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ ì œí•œ (0=ì „ì²´)")
    parser.add_argument("--skip", type=int, default=0, help="ì²˜ìŒ Nê°œ íŒŒì¼ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--calc-method", default="Frayn", choices=["Frayn", "Peronnet", "Jeukendrup"])
    parser.add_argument("--smoothing", type=int, default=10, help="Smoothing window í¬ê¸°")
    args = parser.parse_args()
    
    print("=" * 60)
    print("CPET ë°ì´í„° ì¼ê´„ ì„í¬íŠ¸")
    print("=" * 60)
    
    # Excel íŒŒì¼ ìˆ˜ì§‘
    if not CPET_DATA_DIR.exists():
        print(f"âŒ CPET_data ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {CPET_DATA_DIR}")
        return
    
    excel_files = collect_excel_files(CPET_DATA_DIR)
    total_files = len(excel_files)
    print(f"\nğŸ“ ë°œê²¬ëœ Excel íŒŒì¼: {total_files}ê°œ")
    
    if args.skip > 0:
        excel_files = excel_files[args.skip:]
        print(f"   (--skip {args.skip} ì ìš©, {len(excel_files)}ê°œ ë‚¨ìŒ)")
    
    if args.limit > 0:
        excel_files = excel_files[:args.limit]
        print(f"   (--limit {args.limit} ì ìš©)")
    
    # í”¼í—˜ìë³„ ê·¸ë£¹í•‘
    subjects_files = {}
    for file_path in excel_files:
        last_name, first_name, research_id, test_date = extract_subject_info(file_path.name)
        if research_id:
            if research_id not in subjects_files:
                subjects_files[research_id] = []
            subjects_files[research_id].append({
                "path": file_path,
                "date": test_date,
                "name": f"{first_name} {last_name}"
            })
    
    print(f"\nğŸ‘¥ í”¼í—˜ì: {len(subjects_files)}ëª…")
    for research_id, files in subjects_files.items():
        print(f"   - {research_id}: {len(files)}ê°œ íŒŒì¼")
    
    if args.dry_run:
        print("\nğŸ” --dry-run ëª¨ë“œ: ì‹¤ì œ ì—…ë¡œë“œë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("\níŒŒì¼ ëª©ë¡ (Excel ë‚´ë¶€ ì •ë³´ ê¸°ë°˜):")
        for i, file_path in enumerate(excel_files, 1):
            excel_info = extract_subject_from_excel(file_path)
            last_name = excel_info.get('last_name', '?')
            first_name = excel_info.get('first_name', '?')
            _, _, _, test_date = extract_subject_info(file_path.name)
            date_str = test_date.strftime("%Y-%m-%d") if test_date else "unknown"
            print(f"  {i:3}. [{first_name} {last_name}] {date_str} - {file_path.name}")
        return
    
    # API ì„œë²„ í™•ì¸
    print("\nğŸ”Œ API ì„œë²„ ì—°ê²° í™•ì¸...")
    
    async with httpx.AsyncClient() as client:
        try:
            health = await client.get(f"{API_BASE_URL.replace('/api', '')}/health", timeout=5.0)
            if health.status_code != 200:
                print(f"âŒ API ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                print(f"   cd backend && python -m uvicorn app.main:app --reload --port 8100")
                return
        except Exception as e:
            print(f"âŒ API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            print(f"   cd backend && python -m uvicorn app.main:app --reload --port 8100")
            return
        
        print("âœ… API ì„œë²„ ì—°ê²°ë¨")
        
        # ë¡œê·¸ì¸
        print("\nğŸ” ê´€ë¦¬ì ë¡œê·¸ì¸...")
        token = await get_auth_token(client)
        if not token:
            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨. ê´€ë¦¬ì ê³„ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        print("âœ… ë¡œê·¸ì¸ ì„±ê³µ")
        
        # Subject ID ìºì‹œ (ì¤‘ë³µ ì¡°íšŒ ë°©ì§€)
        subject_cache = {}
        
        # ì—…ë¡œë“œ ì‹œì‘
        print(f"\nğŸ“¤ ì—…ë¡œë“œ ì‹œì‘ (ì´ {len(excel_files)}ê°œ íŒŒì¼)")
        if args.skip > 0:
            print(f"   (íŒŒì¼ #{args.skip + 1}ë¶€í„° ì‹œì‘)")
        print("-" * 60)
        
        results = {
            "success": [],
            "failed": [],
            "skipped": []
        }
        
        start_idx = args.skip  # ì›ë˜ ì¸ë±ìŠ¤ ì¶”ì 
        for i, file_path in enumerate(excel_files, 1):
            original_idx = start_idx + i  # ì „ì²´ íŒŒì¼ ëª©ë¡ì—ì„œì˜ ì¸ë±ìŠ¤
            
            # Excel íŒŒì¼ ë‚´ë¶€ì—ì„œ í”¼í—˜ì ì •ë³´ ì¶”ì¶œ
            excel_info = extract_subject_from_excel(file_path)
            last_name = excel_info.get('last_name', '')
            first_name = excel_info.get('first_name', '')
            
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            _, _, _, test_date = extract_subject_info(file_path.name)
            date_str = test_date.strftime("%Y-%m-%d") if test_date else "unknown"
            
            print(f"\n[{original_idx}/{total_files}] {file_path.name}")
            print(f"   í”¼í—˜ì (Excel): {first_name} {last_name}")
            print(f"   ë‚ ì§œ: {date_str}")
            
            if not last_name or not first_name:
                print(f"   âš ï¸ Excelì—ì„œ í”¼í—˜ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                results["skipped"].append({
                    "file": file_path.name,
                    "reason": "Subject info not found in Excel"
                })
                continue
            
            # Subject ID ì¡°íšŒ (ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰, ìºì‹œ ì‚¬ìš©)
            subject_id = await get_subject_by_name(client, token, last_name, first_name, subject_cache)
            if not subject_id:
                print(f"   âš ï¸ DBì—ì„œ í”¼í—˜ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {first_name} {last_name}")
                results["skipped"].append({
                    "file": file_path.name,
                    "reason": f"Subject not found in DB: {first_name} {last_name}"
                })
                continue
            
            # ì—…ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            print(f"   ğŸ“¤ ì—…ë¡œë“œ ì¤‘...")
            success, test_id, info = await upload_file_with_retry(
                token, file_path, subject_id,
                calc_method=args.calc_method,
                smoothing_window=args.smoothing
            )
            
            if success:
                print(f"   âœ… ì„±ê³µ! test_id: {test_id}")
                results["success"].append({
                    "file": file_path.name,
                    "test_id": test_id,
                    "subject": f"{first_name} {last_name}",
                    "date": date_str
                })
            else:
                print(f"   âŒ ì‹¤íŒ¨: {info}")
                results["failed"].append({
                    "file": file_path.name,
                    "error": info
                })
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ (2ì´ˆ)
            await asyncio.sleep(2.0)
        
        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ“Š ì—…ë¡œë“œ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"   âœ… ì„±ê³µ: {len(results['success'])}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {len(results['failed'])}ê°œ")
        print(f"   âš ï¸ ìŠ¤í‚µ: {len(results['skipped'])}ê°œ")
        
        # ê²°ê³¼ ì €ì¥
        result_file = Path(__file__).parent / "import_results.json"
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥: {result_file}")
        
        if results["failed"]:
            print("\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼:")
            for item in results["failed"]:
                print(f"   - {item['file']}: {item['error']}")


if __name__ == "__main__":
    asyncio.run(main())
